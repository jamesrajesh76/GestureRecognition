{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWswvFz2B4b3"
   },
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2dFdbLpPB4b9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from skimage.transform import resize\n",
    "import imageio.v2 as imageio\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3xk5zTXB4cS"
   },
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "W0ivMmOpB4cU",
    "outputId": "de5c2eb9-9dd4-468c-e2ff-680eb77022f2"
   },
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4RWHE-fB4cW"
   },
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WN8glHaLB4cY"
   },
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('Project_data/val.csv').readlines())\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running on local CPU.. hence batch size fixed at 32.\n",
    "    - Sequence (total videos 663) and # of full Batch (663/batch size 32 = 20) \n",
    "    - # of Videos in final batch (left over videos after full batchs : 663 minus 640 = 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "663\n",
      "20\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the order of Videos (not frames)\n",
    "\n",
    "t = np.random.permutation(train_doc)\n",
    "num_batches = int(len(t)/32)\n",
    "final_batch = len(t)%batch_size\n",
    "print(len(t))   #  Sequence - # of videos\n",
    "print(num_batches) \n",
    "print(final_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Images (video resolution) of different size (360x360, 120x160), Resized to smaller resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6bLU5_9B4cZ"
   },
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with some of the parts of the generator function such that you get high accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on smaller subset of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uC_ksZ3XB4ca"
   },
   "outputs": [],
   "source": [
    "def generator_8(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "\n",
    "# choose the required number of images from each video - more the images considered longer the training period\n",
    "    img_idx = [1, 5, 10, 15, 20, 22, 25, 29 ]  \n",
    "\n",
    "# generate data till the last epoch\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = int(len(t)/batch_size)\n",
    "        for batch in range(num_batches):\n",
    "\n",
    "# batch_data holds the number of images mentioned in batch size\n",
    "            batch_data = np.zeros((batch_size,8,84,84,3)) # batch_size, 3D resolution, channel \n",
    "\n",
    "# batch label holds the class of the image corresponding to the image in batch_data\n",
    "            batch_labels = np.zeros((batch_size,5))\n",
    "            for folder in range(batch_size):\n",
    "                \n",
    "# List all Image file names in the current folder \n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0])\n",
    "                #print(imgs)\n",
    "                for idx,item in enumerate(img_idx):\n",
    "                    image = imageio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "\n",
    "# crop : only the 120x160 images will be procesesed by the below crop operation\n",
    "                    if image.shape[0] != image.shape[1]:\n",
    "                        image = image[0:120, 10:150]\n",
    "                    else:\n",
    "                        image = image\n",
    "\n",
    "# resize : crop operation performed above will convert 120x160 to 120x140.\n",
    "# With the images that are 120x140 fetch 120x120 pixels and resize them to 84x84\n",
    "\n",
    "                    if image.shape[1] == 140:  \n",
    "                        image = resize(image[:,10:130,:],(84,84)).astype(np.float32)\n",
    "                    else:\n",
    "                        image = resize(image,(84,84)).astype(np.float32)\n",
    "\n",
    "# Normalize RGB channel data\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0] - 104\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1] - 117\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2] - 123\n",
    "\n",
    "# apply class label as binary OHT [0. 1. 0. 0. 0.]                   \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels\n",
    "\n",
    "# Final batch with left over videos after processing all the FULL batches\n",
    "\n",
    "        if (len(t)%batch_size) != 0:\n",
    "            batch_data = np.zeros((len(t)%batch_size,8,84,84,3))\n",
    "            batch_labels = np.zeros((len(t)%batch_size,5))\n",
    "            for folder in range(len(t)%batch_size):\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*batch_size)].split(';')[0])\n",
    "                for idx,item in enumerate(img_idx):\n",
    "                    image = imageio.imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "\n",
    "                    if image.shape[0] != image.shape[1]:\n",
    "                        image = image[0:120, 10:150]\n",
    "                    else:\n",
    "                        image = image\n",
    "                        \n",
    "                        \n",
    "                    if image.shape[1] == 140:\n",
    "                        image = resize(image[:,10:130,:],(84,84)).astype(np.float32)\n",
    "                    else:\n",
    "                        image = resize(image,(84,84)).astype(np.float32)\n",
    "\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0] - 104\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1] - 117\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2] - 123\n",
    "\n",
    "                batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1\n",
    "\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "OKusyUJcB4cd",
    "outputId": "42d7c299-51a1-49ad-8eb5-cf80f6b949d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 10\n"
     ]
    }
   ],
   "source": [
    "# Fetch date/time to create a folder to save h5 files which can later be loaded to test the model performance\n",
    "\n",
    "curr_dt_time = datetime.datetime.now()\n",
    "\n",
    "train_path = 'Project_data/train'\n",
    "val_path = 'Project_data/val'\n",
    "source_path=train_path\n",
    "\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 10\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OELVUTqBB4ce"
   },
   "source": [
    "## Conv3D Model\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D`. Also remember that the last layer is the softmax. Remember that the network is designed in such a way that the model is able to fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ebqwqV_yB4cf"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Hidden Layer 1\n",
    "model.add(Conv3D(64, (3,3,3), strides=(1,1,1), padding='same', input_shape=(8,84,84,3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,1), strides=(2,2,1)))\n",
    "\n",
    "# Hidden Layer 2\n",
    "model.add(Conv3D(128, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Hidden Layer 3\n",
    "model.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Hidden Layer 4\n",
    "model.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2), padding='same'))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(512, activation='elu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Dense to the 5 gesture classes\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4yeetfCB4ct"
   },
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "RI0za7kgB4c2",
    "outputId": "09469b94-2990-437b-c2de-6156c8547991"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d (Conv3D)             (None, 8, 84, 84, 64)     5248      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 8, 84, 84, 64)    256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 8, 84, 84, 64)     0         \n",
      "                                                                 \n",
      " max_pooling3d (MaxPooling3D  (None, 4, 42, 84, 64)    0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv3d_1 (Conv3D)           (None, 4, 42, 84, 128)    221312    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 4, 42, 84, 128)   512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 4, 42, 84, 128)    0         \n",
      "                                                                 \n",
      " max_pooling3d_1 (MaxPooling  (None, 2, 21, 42, 128)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2, 21, 42, 128)    0         \n",
      "                                                                 \n",
      " conv3d_2 (Conv3D)           (None, 2, 21, 42, 256)    884992    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 2, 21, 42, 256)   1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 2, 21, 42, 256)    0         \n",
      "                                                                 \n",
      " max_pooling3d_2 (MaxPooling  (None, 1, 10, 21, 256)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1, 10, 21, 256)    0         \n",
      "                                                                 \n",
      " conv3d_3 (Conv3D)           (None, 1, 10, 21, 256)    1769728   \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 1, 10, 21, 256)   1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 1, 10, 21, 256)    0         \n",
      "                                                                 \n",
      " max_pooling3d_3 (MaxPooling  (None, 1, 5, 11, 256)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 14080)             0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 14080)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               7209472   \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,096,133\n",
      "Trainable params: 10,094,725\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\admin\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Setting faster learning rate\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.7, nesterov=True)\n",
    "\n",
    "#Compile Model\n",
    "\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nU3mGWOB4c6"
   },
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4Yp3XgkbB4c7"
   },
   "outputs": [],
   "source": [
    "train_generator = generator_8(train_path, train_doc, batch_size)\n",
    "val_generator = generator_8(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "fdAfbtLWB4c7",
    "outputId": "1715607d-6400-4231-db75-eeee50068dce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    }
   ],
   "source": [
    "# Create folder to save H5 files generated for each epoch (save_best_only=False)\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "\n",
    "# Save H5 file with loss, acc, val loss, val acc metrics\n",
    "\n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "# check point : save H5 file after each epoch\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "# Reduce Learning Rate (overfitting) when model Platueaus \n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, mode='min', epsilon=0.001, cooldown=0, min_lr=0.0001)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9SxnCT9IB4c8"
   },
   "outputs": [],
   "source": [
    "# Training Epoch - # of steps. Increment the step value by one when there is residual videos after FULL batch\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "# Validation Epoch - # of steps\n",
    "    \n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZUamCNJB4dA"
   },
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5fCt_eSFB4dM",
    "outputId": "dec7c59f-5346-43db-f9d5-a8f45703da14",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_8128\\1316291930.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 32\n",
      "Epoch 1/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 6.1439 - categorical_accuracy: 0.3137 Source path =  Project_data/val ; batch size = 32\n",
      "\n",
      "Epoch 1: saving model to model_init_2023-04-1012_52_02.259878\\model-00001-6.14393-0.31373-89.40958-0.23000.h5\n",
      "21/21 [==============================] - 968s 46s/step - loss: 6.1439 - categorical_accuracy: 0.3137 - val_loss: 89.4096 - val_categorical_accuracy: 0.2300 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.7539 - categorical_accuracy: 0.4525 \n",
      "Epoch 2: saving model to model_init_2023-04-1012_52_02.259878\\model-00002-1.75390-0.45249-15.91531-0.22000.h5\n",
      "21/21 [==============================] - 970s 46s/step - loss: 1.7539 - categorical_accuracy: 0.4525 - val_loss: 15.9153 - val_categorical_accuracy: 0.2200 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.6876 - categorical_accuracy: 0.4449 \n",
      "Epoch 3: saving model to model_init_2023-04-1012_52_02.259878\\model-00003-1.68761-0.44495-17.80415-0.21000.h5\n",
      "21/21 [==============================] - 978s 47s/step - loss: 1.6876 - categorical_accuracy: 0.4449 - val_loss: 17.8041 - val_categorical_accuracy: 0.2100 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.3447 - categorical_accuracy: 0.5309 \n",
      "Epoch 4: saving model to model_init_2023-04-1012_52_02.259878\\model-00004-1.34469-0.53092-5.56814-0.26000.h5\n",
      "21/21 [==============================] - 1012s 48s/step - loss: 1.3447 - categorical_accuracy: 0.5309 - val_loss: 5.5681 - val_categorical_accuracy: 0.2600 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.3696 - categorical_accuracy: 0.5189 \n",
      "Epoch 5: saving model to model_init_2023-04-1012_52_02.259878\\model-00005-1.36958-0.51885-3.72721-0.32000.h5\n",
      "21/21 [==============================] - 1037s 50s/step - loss: 1.3696 - categorical_accuracy: 0.5189 - val_loss: 3.7272 - val_categorical_accuracy: 0.3200 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.1238 - categorical_accuracy: 0.6199 \n",
      "Epoch 6: saving model to model_init_2023-04-1012_52_02.259878\\model-00006-1.12376-0.61991-2.34655-0.41000.h5\n",
      "21/21 [==============================] - 1029s 49s/step - loss: 1.1238 - categorical_accuracy: 0.6199 - val_loss: 2.3466 - val_categorical_accuracy: 0.4100 - lr: 0.0100\n",
      "Epoch 7/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9534 - categorical_accuracy: 0.6546 \n",
      "Epoch 7: saving model to model_init_2023-04-1012_52_02.259878\\model-00007-0.95341-0.65460-1.59084-0.49000.h5\n",
      "21/21 [==============================] - 1059s 50s/step - loss: 0.9534 - categorical_accuracy: 0.6546 - val_loss: 1.5908 - val_categorical_accuracy: 0.4900 - lr: 0.0100\n",
      "Epoch 8/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9923 - categorical_accuracy: 0.6712 \n",
      "Epoch 8: saving model to model_init_2023-04-1012_52_02.259878\\model-00008-0.99232-0.67119-1.07997-0.65000.h5\n",
      "21/21 [==============================] - 1012s 48s/step - loss: 0.9923 - categorical_accuracy: 0.6712 - val_loss: 1.0800 - val_categorical_accuracy: 0.6500 - lr: 0.0100\n",
      "Epoch 9/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9541 - categorical_accuracy: 0.6561 \n",
      "Epoch 9: saving model to model_init_2023-04-1012_52_02.259878\\model-00009-0.95414-0.65611-0.96637-0.66000.h5\n",
      "21/21 [==============================] - 995s 47s/step - loss: 0.9541 - categorical_accuracy: 0.6561 - val_loss: 0.9664 - val_categorical_accuracy: 0.6600 - lr: 0.0100\n",
      "Epoch 10/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8911 - categorical_accuracy: 0.6968 \n",
      "Epoch 10: saving model to model_init_2023-04-1012_52_02.259878\\model-00010-0.89115-0.69683-1.39683-0.49000.h5\n",
      "21/21 [==============================] - 1001s 48s/step - loss: 0.8911 - categorical_accuracy: 0.6968 - val_loss: 1.3968 - val_categorical_accuracy: 0.4900 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b3d439e640>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, \n",
    "                    validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add more images to the training ( 8 to 18 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "uC_ksZ3XB4ca"
   },
   "outputs": [],
   "source": [
    "def generator_18(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "\n",
    "# choose the required number of images from each video - more the images considered longer the training period\n",
    "    img_idx = [0,1,2,4,6,8,10,12,14,16,18,20,22,24,26,27,28,29] # [1, 5, 10, 15, 20, 22, 25, 29 ]  \n",
    "\n",
    "# generate data till the last epoch\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = int(len(t)/batch_size)\n",
    "        for batch in range(num_batches):\n",
    "\n",
    "# batch_data holds the number of images mentioned in batch size\n",
    "            batch_data = np.zeros((batch_size,18,84,84,3)) # batch_size, 3D resolution, channel \n",
    "\n",
    "# batch label holds the class of the image corresponding to the image in batch_data\n",
    "            batch_labels = np.zeros((batch_size,5))\n",
    "            for folder in range(batch_size):\n",
    "                \n",
    "# List all Image file names in the current folder \n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0])\n",
    "                #print(imgs)\n",
    "                for idx,item in enumerate(img_idx):\n",
    "                    image = imageio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "\n",
    "# crop : only the 120x160 images will be procesesed by the below crop operation\n",
    "                    if image.shape[0] != image.shape[1]:\n",
    "                        image = image[0:120, 10:150]\n",
    "                    else:\n",
    "                        image = image\n",
    "\n",
    "# resize : crop operation performed above will convert 120x160 to 120x140.\n",
    "# With the images that are 120x140 fetch 120x120 pixels and resize them to 84x84\n",
    "\n",
    "                    if image.shape[1] == 140:  \n",
    "                        image = resize(image[:,10:130,:],(84,84)).astype(np.float32)\n",
    "                    else:\n",
    "                        image = resize(image,(84,84)).astype(np.float32)\n",
    "\n",
    "# Normalize RGB channel data\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0] - 104\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1] - 117\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2] - 123\n",
    "\n",
    "# apply class label as binary OHT [0. 1. 0. 0. 0.]                   \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels\n",
    "\n",
    "# Final batch with left over videos after processing all the FULL batches\n",
    "\n",
    "        if (len(t)%batch_size) != 0:\n",
    "            batch_data = np.zeros((len(t)%batch_size,18,84,84,3))\n",
    "            batch_labels = np.zeros((len(t)%batch_size,5))\n",
    "            for folder in range(len(t)%batch_size):\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*batch_size)].split(';')[0])\n",
    "                for idx,item in enumerate(img_idx):\n",
    "                    image = imageio.imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "\n",
    "                    if image.shape[0] != image.shape[1]:\n",
    "                        image = image[0:120, 10:150]\n",
    "                    else:\n",
    "                        image = image\n",
    "                        \n",
    "                        \n",
    "                    if image.shape[1] == 140:\n",
    "                        image = resize(image[:,10:130,:],(84,84)).astype(np.float32)\n",
    "                    else:\n",
    "                        image = resize(image,(84,84)).astype(np.float32)\n",
    "\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0] - 104\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1] - 117\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2] - 123\n",
    "\n",
    "                batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1\n",
    "\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "OKusyUJcB4cd",
    "outputId": "42d7c299-51a1-49ad-8eb5-cf80f6b949d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 10\n"
     ]
    }
   ],
   "source": [
    "# Fetch date/time to create a folder to save h5 files which can later be loaded to test the model performance\n",
    "\n",
    "curr_dt_time = datetime.datetime.now()\n",
    "\n",
    "train_path = 'Project_data/train'\n",
    "val_path = 'Project_data/val'\n",
    "source_path=train_path\n",
    "\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 10\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator_18(train_path, train_doc, batch_size)\n",
    "val_generator = generator_18(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    }
   ],
   "source": [
    "# Create folder to save H5 files generated for each epoch (save_best_only=False)\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "\n",
    "# Save H5 file with loss, acc, val loss, val acc metrics\n",
    "\n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "# check point : save H5 file after each epoch\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "# Reduce Learning Rate (overfitting) when model Platueaus \n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, mode='min', epsilon=0.001, cooldown=0, min_lr=0.0001)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Hidden Layer 1\n",
    "model.add(Conv3D(64, (3,3,3), strides=(1,1,1), padding='same', input_shape=(18,84,84,3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,1), strides=(2,2,1)))\n",
    "\n",
    "# Hidden Layer 2\n",
    "model.add(Conv3D(128, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Hidden Layer 3\n",
    "model.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Hidden Layer 4\n",
    "model.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2), padding='same'))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(512, activation='elu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Dense to the 5 gesture classes\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_4 (Conv3D)           (None, 18, 84, 84, 64)    5248      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 18, 84, 84, 64)   256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 18, 84, 84, 64)    0         \n",
      "                                                                 \n",
      " max_pooling3d_4 (MaxPooling  (None, 9, 42, 84, 64)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_5 (Conv3D)           (None, 9, 42, 84, 128)    221312    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 9, 42, 84, 128)   512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 9, 42, 84, 128)    0         \n",
      "                                                                 \n",
      " max_pooling3d_5 (MaxPooling  (None, 4, 21, 42, 128)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 4, 21, 42, 128)    0         \n",
      "                                                                 \n",
      " conv3d_6 (Conv3D)           (None, 4, 21, 42, 256)    884992    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 4, 21, 42, 256)   1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 4, 21, 42, 256)    0         \n",
      "                                                                 \n",
      " max_pooling3d_6 (MaxPooling  (None, 2, 10, 21, 256)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 2, 10, 21, 256)    0         \n",
      "                                                                 \n",
      " conv3d_7 (Conv3D)           (None, 2, 10, 21, 256)    1769728   \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 2, 10, 21, 256)   1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 2, 10, 21, 256)    0         \n",
      "                                                                 \n",
      " max_pooling3d_7 (MaxPooling  (None, 1, 5, 11, 256)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 14080)             0         \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 14080)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               7209472   \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,096,133\n",
      "Trainable params: 10,094,725\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Setting faster learning rate\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.7, nesterov=True)\n",
    "\n",
    "#Compile Model\n",
    "\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_8128\\1316291930.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 10.8819 - categorical_accuracy: 0.2715  Source path =  Project_data/val ; batch size = 32\n",
      "\n",
      "Epoch 1: saving model to model_init_2023-04-1015_39_45.782359\\model-00001-10.88193-0.27149-36.66798-0.18000.h5\n",
      "21/21 [==============================] - 2171s 103s/step - loss: 10.8819 - categorical_accuracy: 0.2715 - val_loss: 36.6680 - val_categorical_accuracy: 0.1800 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 2.3349 - categorical_accuracy: 0.3544  \n",
      "Epoch 2: saving model to model_init_2023-04-1015_39_45.782359\\model-00002-2.33492-0.35445-37.17839-0.18000.h5\n",
      "21/21 [==============================] - 2181s 103s/step - loss: 2.3349 - categorical_accuracy: 0.3544 - val_loss: 37.1784 - val_categorical_accuracy: 0.1800 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 2.2430 - categorical_accuracy: 0.3771  \n",
      "Epoch 3: saving model to model_init_2023-04-1015_39_45.782359\\model-00003-2.24298-0.37707-18.78424-0.24000.h5\n",
      "21/21 [==============================] - 2181s 104s/step - loss: 2.2430 - categorical_accuracy: 0.3771 - val_loss: 18.7842 - val_categorical_accuracy: 0.2400 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.8830 - categorical_accuracy: 0.4419  \n",
      "Epoch 4: saving model to model_init_2023-04-1015_39_45.782359\\model-00004-1.88303-0.44193-7.50042-0.23000.h5\n",
      "21/21 [==============================] - 2347s 112s/step - loss: 1.8830 - categorical_accuracy: 0.4419 - val_loss: 7.5004 - val_categorical_accuracy: 0.2300 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.8120 - categorical_accuracy: 0.4676  \n",
      "Epoch 5: saving model to model_init_2023-04-1015_39_45.782359\\model-00005-1.81198-0.46757-7.70416-0.26000.h5\n",
      "21/21 [==============================] - 2236s 107s/step - loss: 1.8120 - categorical_accuracy: 0.4676 - val_loss: 7.7042 - val_categorical_accuracy: 0.2600 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.4686 - categorical_accuracy: 0.5173  \n",
      "Epoch 6: saving model to model_init_2023-04-1015_39_45.782359\\model-00006-1.46859-0.51735-2.40618-0.43000.h5\n",
      "21/21 [==============================] - 2316s 110s/step - loss: 1.4686 - categorical_accuracy: 0.5173 - val_loss: 2.4062 - val_categorical_accuracy: 0.4300 - lr: 0.0100\n",
      "Epoch 7/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.5727 - categorical_accuracy: 0.5173  \n",
      "Epoch 7: saving model to model_init_2023-04-1015_39_45.782359\\model-00007-1.57269-0.51735-1.54447-0.57000.h5\n",
      "21/21 [==============================] - 2362s 113s/step - loss: 1.5727 - categorical_accuracy: 0.5173 - val_loss: 1.5445 - val_categorical_accuracy: 0.5700 - lr: 0.0100\n",
      "Epoch 8/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.3421 - categorical_accuracy: 0.5807  \n",
      "Epoch 8: saving model to model_init_2023-04-1015_39_45.782359\\model-00008-1.34206-0.58069-1.05020-0.65000.h5\n",
      "21/21 [==============================] - 2123s 101s/step - loss: 1.3421 - categorical_accuracy: 0.5807 - val_loss: 1.0502 - val_categorical_accuracy: 0.6500 - lr: 0.0100\n",
      "Epoch 9/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.3424 - categorical_accuracy: 0.5611  \n",
      "Epoch 9: saving model to model_init_2023-04-1015_39_45.782359\\model-00009-1.34240-0.56109-0.97665-0.65000.h5\n",
      "21/21 [==============================] - 2129s 101s/step - loss: 1.3424 - categorical_accuracy: 0.5611 - val_loss: 0.9766 - val_categorical_accuracy: 0.6500 - lr: 0.0100\n",
      "Epoch 10/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.1560 - categorical_accuracy: 0.6063  \n",
      "Epoch 10: saving model to model_init_2023-04-1015_39_45.782359\\model-00010-1.15602-0.60633-1.14883-0.54000.h5\n",
      "21/21 [==============================] - 2437s 116s/step - loss: 1.1560 - categorical_accuracy: 0.6063 - val_loss: 1.1488 - val_categorical_accuracy: 0.5400 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b382a05d30>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, \n",
    "                    validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Neural+Networks+Project+Conv3D.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
